{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets transformers scikit-learn pandas numpy\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"14\"\n",
    "torch.set_num_threads(14)\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (set_seed, AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer)\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import (confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, \n",
    "                             average_precision_score, matthews_corrcoef, precision_recall_fscore_support)\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDbDistillationPipeline:\n",
    "    \"\"\"\n",
    "    The class sets up a pipeline for:\n",
    "      1) Loading & splitting IMDb data (train, validation, test).\n",
    "      2) Fine-tuning a teacher model (BERT).\n",
    "      3) Distilling to a student model (DistilBERT).\n",
    "      4) Comparing the results of the Teacher vs. Student.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, val_size=0.2):\n",
    "        self.val_size = val_size\n",
    "        self.raw_data = None\n",
    "        self.imdb_data = None\n",
    "        self.teacher_model = None\n",
    "        self.teacher_tokenizer = None\n",
    "        self.teacher_dataset = None\n",
    "        self.teacher_test_metrics = None\n",
    "        \n",
    "        self.student_model = None\n",
    "        self.student_tokenizer = None\n",
    "        self.student_dataset = None\n",
    "        self.distil_dataset = None\n",
    "        self.student_test_metrics = None\n",
    "\n",
    "    def load_and_split_imdb(self):\n",
    "        \"\"\"\n",
    "        This function loads the IMDb dataset from Hugging Face and creates a train/val/test dataset.\n",
    "        \"\"\"\n",
    "        raw = load_dataset(\"imdb\")\n",
    "        #Split original train into (train + val)\n",
    "        split_train = raw[\"train\"].train_test_split(test_size=self.val_size, seed=SEED)\n",
    "        #Construct new DatasetDict with the validation set\n",
    "        self.imdb_data = DatasetDict({\n",
    "            \"train\": split_train[\"train\"],\n",
    "            \"validation\": split_train[\"test\"],\n",
    "            \"test\": raw[\"test\"]\n",
    "        })\n",
    "        print(f\"Train size: {len(self.imdb_data['train'])}, \"\n",
    "              f\"Val size: {len(self.imdb_data['validation'])}, \"\n",
    "              f\"Test size: {len(self.imdb_data['test'])}\")\n",
    "\n",
    "    def setup_teacher(self, model_name=\"bert-base-uncased\"):\n",
    "        \"\"\"\n",
    "        This function loads the teacher tokenizer & model.\n",
    "        \"\"\"\n",
    "        self.teacher_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.teacher_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, num_labels=2\n",
    "        )\n",
    "\n",
    "    def tokenizer_for_teacher(self, examples):\n",
    "        \"\"\"\n",
    "        This is the tokenizer function for the Teacher model.\n",
    "        \"\"\"\n",
    "        return self.teacher_tokenizer(\n",
    "            examples[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=256\n",
    "        )\n",
    "\n",
    "    def prepare_teacher_data(self):\n",
    "        \"\"\"\n",
    "        This function maps tokenization over the train, validation, test sets with multi-processing.\n",
    "        It renames 'label' -> 'labels', removes 'text', and sets the format to torch.\n",
    "        \"\"\"\n",
    "        t_data = self.imdb_data.map(\n",
    "            self.tokenizer_for_teacher,\n",
    "            batched=True,\n",
    "            num_proc=14\n",
    "        )\n",
    "        t_data = t_data.rename_column(\"label\", \"labels\")\n",
    "        t_data = t_data.remove_columns([\"text\"])\n",
    "        t_data.set_format(\"torch\")\n",
    "        self.teacher_dataset = t_data\n",
    "\n",
    "    def compute_metrics_teacher(self, eval_pred):\n",
    "        \"\"\"\n",
    "        This function is to simply compute the Teacher's metrics such as \n",
    "        accuracy, precision, recall, and f1.\n",
    "        We can add more if needed.\n",
    "        \"\"\"\n",
    "        logits, labels = eval_pred\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        return {\n",
    "            \"eval_accuracy\": acc,\n",
    "            \"eval_precision\": precision,\n",
    "            \"eval_recall\": recall,\n",
    "            \"eval_f1\": f1\n",
    "        }\n",
    "\n",
    "    def train_teacher(self, output_dir=\"./teacher_model_final\", epochs=1):\n",
    "        \"\"\"\n",
    "        This function is to train the Teacher model.\n",
    "        \"\"\"\n",
    "        teacher_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"no\",\n",
    "            num_train_epochs=epochs,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=8,\n",
    "            learning_rate=5e-5,\n",
    "            logging_steps=100,\n",
    "            disable_tqdm=False, #show progress bar\n",
    "            seed=SEED\n",
    "        )\n",
    "        teacher_trainer = Trainer(\n",
    "            model=self.teacher_model,\n",
    "            args=teacher_args,\n",
    "            train_dataset=self.teacher_dataset[\"train\"],\n",
    "            eval_dataset=self.teacher_dataset[\"validation\"],\n",
    "            compute_metrics=self.compute_metrics_teacher\n",
    "        )\n",
    "\n",
    "        teacher_trainer.train()\n",
    "        val_metrics = teacher_trainer.evaluate(self.teacher_dataset[\"validation\"])\n",
    "        test_metrics = teacher_trainer.evaluate(self.teacher_dataset[\"test\"])\n",
    "        self.teacher_test_metrics = test_metrics\n",
    "        self.teacher_model.save_pretrained(output_dir)\n",
    "        self.teacher_tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "        print(\"\\nTeacher Validation Metrics:\", val_metrics)\n",
    "        print(\"Teacher Test Metrics:\", test_metrics)\n",
    "\n",
    "    def setup_student(self, model_name=\"distilbert-base-uncased\"):\n",
    "        \"\"\"\n",
    "        This function loads the Student tokenizer & model.\n",
    "        \"\"\"\n",
    "        self.student_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.student_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, num_labels=2\n",
    "        )\n",
    "\n",
    "    def tokenizer_for_student(self, examples):\n",
    "        \"\"\"\n",
    "        This is the tokenizer function for the Student model.\n",
    "        \"\"\"\n",
    "        return self.student_tokenizer(\n",
    "            examples[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=256\n",
    "        )\n",
    "\n",
    "    def prepare_student_data(self):\n",
    "        \"\"\"\n",
    "        This function maps tokenization over the train, validation, test sets with multi-processing.\n",
    "        It renames 'label' -> 'labels', removes 'text', and sets the format to torch.\n",
    "        \"\"\"\n",
    "        s_data = self.imdb_data.map(\n",
    "            self.tokenizer_for_student, \n",
    "            batched=True, \n",
    "            num_proc=14\n",
    "        )\n",
    "        s_data = s_data.rename_column(\"label\", \"labels\")\n",
    "        s_data = s_data.remove_columns([\"text\"])\n",
    "        s_data.set_format(\"torch\")\n",
    "        self.student_dataset = s_data\n",
    "\n",
    "    def get_teacher_logits(self, dataset, batch_size=8):\n",
    "        \"\"\"\n",
    "        The function is to get the teacher logits for each sample in 'dataset'.\n",
    "        It returns a 2D torch.Tensor: [num_samples, num_labels].\n",
    "        \"\"\"\n",
    "        loader = DataLoader(dataset, batch_size=batch_size)\n",
    "        all_logits = []\n",
    "        self.teacher_model.eval()\n",
    "        for batch in loader:\n",
    "            with torch.no_grad():\n",
    "                out = self.teacher_model(\n",
    "                    input_ids=batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"]\n",
    "                )\n",
    "            all_logits.append(out.logits.cpu())\n",
    "        return torch.cat(all_logits, dim=0)\n",
    "\n",
    "    def create_distil_dataset(self):\n",
    "        \"\"\"\n",
    "        This function creates a new dataset dict (train + val + test), \n",
    "        but only the train data has the 'teacher_logits' column for the knowledge distillation, the other ones do not.\n",
    "        \"\"\"\n",
    "        teacher_logits_train = self.get_teacher_logits(self.student_dataset[\"train\"], batch_size=8)\n",
    "        logits_list = teacher_logits_train.numpy().tolist()\n",
    "        train_with_logits = self.student_dataset[\"train\"].add_column(\"teacher_logits\", logits_list)\n",
    "        \n",
    "        self.distil_dataset = DatasetDict({\n",
    "            \"train\": train_with_logits,\n",
    "            \"validation\": self.student_dataset[\"validation\"], #no teacher_logits\n",
    "            \"test\": self.student_dataset[\"test\"] #no teacher_logits\n",
    "        })\n",
    "\n",
    "\n",
    "    class DistillationTrainer(Trainer):\n",
    "        \"\"\"\n",
    "        This is a trainer that does knowledge distillation if 'teacher_logits' is in the batch,\n",
    "        else we will do standard cross entropy for for the val/test sets.\n",
    "        \"\"\"\n",
    "        def __init__(self, alpha=0.5, temperature=2.0, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            self.alpha = alpha\n",
    "            self.temperature = temperature\n",
    "\n",
    "        def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "            labels = inputs[\"labels\"]\n",
    "            outputs = model(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"]\n",
    "            )\n",
    "            student_logits = outputs.logits\n",
    "            \n",
    "            if \"teacher_logits\" in inputs:\n",
    "                teacher_logits = inputs[\"teacher_logits\"].to(model.device)\n",
    "                \n",
    "                ce_loss = torch.nn.functional.cross_entropy(student_logits, labels)\n",
    "                T = self.temperature\n",
    "                student_soft = torch.nn.functional.log_softmax(student_logits / T, dim=-1)\n",
    "                teacher_soft = torch.nn.functional.softmax(teacher_logits / T, dim=-1)\n",
    "                kd_loss = torch.nn.functional.kl_div(\n",
    "                    student_soft, teacher_soft, reduction=\"batchmean\"\n",
    "                ) * (T**2)\n",
    "\n",
    "                loss = self.alpha * kd_loss + (1.0 - self.alpha) * ce_loss\n",
    "                return (loss, outputs) if return_outputs else loss\n",
    "            else:\n",
    "                loss = torch.nn.functional.cross_entropy(student_logits, labels)\n",
    "                return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def compute_metrics_student(self, eval_pred):\n",
    "        \"\"\"\n",
    "        This function is to simply compute the Seacher's metrics such as \n",
    "        accuracy, precision, recall, and f1.\n",
    "        We can add more if needed.\n",
    "        \"\"\"\n",
    "        logits, labels = eval_pred\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            labels, preds, average=\"binary\"\n",
    "        )\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        return {\n",
    "            \"eval_accuracy\": acc,\n",
    "            \"eval_precision\": precision,\n",
    "            \"eval_recall\": recall,\n",
    "            \"eval_f1\": f1\n",
    "        }\n",
    "\n",
    "    def train_student(self, output_dir=\"./student_model_final\", epochs=1, alpha=0.5, temperature=2.0):\n",
    "        \"\"\"\n",
    "        This function is to train the Student with knowledge distillation.\n",
    "        \"\"\"\n",
    "        student_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"no\",\n",
    "            num_train_epochs=epochs,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=8,\n",
    "            learning_rate=5e-5,\n",
    "            logging_steps=100,\n",
    "            disable_tqdm=False,\n",
    "            seed=SEED\n",
    "        )\n",
    "        distil_trainer = self.DistillationTrainer(\n",
    "            alpha=alpha,\n",
    "            temperature=temperature,\n",
    "            model=self.student_model,\n",
    "            args=student_args,\n",
    "            train_dataset=self.distil_dataset[\"train\"],\n",
    "            eval_dataset=self.distil_dataset[\"validation\"],\n",
    "            tokenizer=self.student_tokenizer,\n",
    "            compute_metrics=self.compute_metrics_student\n",
    "        )\n",
    "        distil_trainer.train()\n",
    "        val_metrics = distil_trainer.evaluate(self.distil_dataset[\"validation\"])\n",
    "        test_metrics = distil_trainer.evaluate(self.distil_dataset[\"test\"])\n",
    "        self.student_test_metrics = test_metrics\n",
    "\n",
    "        self.student_model.save_pretrained(output_dir)\n",
    "        self.student_tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "        print(\"\\nStudent Validation Metrics:\", val_metrics)\n",
    "        print(\"Student Test Metrics:\", test_metrics)\n",
    "\n",
    "    def compare_accuracy(self):\n",
    "        \"\"\"\n",
    "        This function is to compare teacher and student test performance.\n",
    "        We are using the 'eval_accuracy' in both dictionaries.\n",
    "        \"\"\"\n",
    "        if self.teacher_test_metrics is None or self.student_test_metrics is None:\n",
    "            print(\"Train teacher & student first.\")\n",
    "            return\n",
    "\n",
    "        t_acc = self.teacher_test_metrics[\"eval_accuracy\"]\n",
    "        s_acc = self.student_test_metrics[\"eval_accuracy\"]\n",
    "        if t_acc == 0:\n",
    "            ratio = 0.0\n",
    "        else:\n",
    "            ratio = (s_acc / t_acc) * 100\n",
    "\n",
    "        print(\"Teacher Test Results:\", self.teacher_test_metrics)\n",
    "        print(\"Student Test Results:\", self.student_test_metrics)\n",
    "        print(f\"The Student model retains about {ratio:.1f}% of the Teacher's accuracy.\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    This function is the main function that runs the entire pipeline for teacher and student.\n",
    "    \"\"\"\n",
    "    pipeline = IMDbDistillationPipeline(val_size=0.2)\n",
    "    #Load & split the data we have,\n",
    "    pipeline.load_and_split_imdb()\n",
    "    #Teacher setup & train from BERT.\n",
    "    pipeline.setup_teacher(\"bert-base-uncased\")\n",
    "    pipeline.prepare_teacher_data()\n",
    "    pipeline.train_teacher(epochs=1)\n",
    "    #Student setup & distill.\n",
    "    pipeline.setup_student(\"distilbert-base-uncased\")\n",
    "    pipeline.prepare_student_data()\n",
    "    pipeline.create_distil_dataset()\n",
    "    pipeline.train_student(epochs=1, alpha=0.5, temperature=2.0)\n",
    "    #Compare the results.\n",
    "    pipeline.compare_accuracy()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "EXAMPLE OUTPUT: \n",
    "\n",
    "Train size: 20000, Val size: 5000, Test size: 25000\n",
    "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "Map (num_proc=14): 100%|██████████| 20000/20000 [00:31<00:00, 644.04 examples/s] \n",
    "Map (num_proc=14): 100%|██████████| 5000/5000 [00:29<00:00, 168.90 examples/s]\n",
    "Map (num_proc=14): 100%|██████████| 25000/25000 [00:32<00:00, 777.77 examples/s] \n",
    "  0%|          | 0/250 [3:04:09<?, ?it/s]\n",
    "  4%|▍         | 100/2500 [08:28<2:58:56,  4.47s/it]{'loss': 0.5869, 'grad_norm': 11.853706359863281, 'learning_rate': 4.8e-05, 'epoch': 0.04}\n",
    "  8%|▊         | 200/2500 [16:54<3:43:54,  5.84s/it]{'loss': 0.4258, 'grad_norm': 9.650397300720215, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.08}\n",
    " 12%|█▏        | 300/2500 [25:48<2:45:48,  4.52s/it]{'loss': 0.4155, 'grad_norm': 16.848670959472656, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.12}\n",
    " 16%|█▌        | 400/2500 [35:02<3:51:42,  6.62s/it]{'loss': 0.3737, 'grad_norm': 13.249711036682129, 'learning_rate': 4.2e-05, 'epoch': 0.16}\n",
    " 20%|██        | 500/2500 [46:08<3:41:54,  6.66s/it]{'loss': 0.3945, 'grad_norm': 13.04289722442627, 'learning_rate': 4e-05, 'epoch': 0.2}\n",
    " 24%|██▍       | 600/2500 [57:26<3:34:35,  6.78s/it]{'loss': 0.364, 'grad_norm': 10.95732307434082, 'learning_rate': 3.8e-05, 'epoch': 0.24}\n",
    " 28%|██▊       | 700/2500 [1:08:49<3:28:37,  6.95s/it]{'loss': 0.3982, 'grad_norm': 0.9471220374107361, 'learning_rate': 3.6e-05, 'epoch': 0.28}\n",
    " 32%|███▏      | 800/2500 [1:20:20<3:16:22,  6.93s/it]{'loss': 0.3302, 'grad_norm': 8.628236770629883, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.32}\n",
    " 36%|███▌      | 900/2500 [1:31:58<3:06:08,  6.98s/it]{'loss': 0.3724, 'grad_norm': 14.761881828308105, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.36}\n",
    " 40%|████      | 1000/2500 [1:40:18<1:57:36,  4.70s/it]{'loss': 0.3738, 'grad_norm': 29.557193756103516, 'learning_rate': 3e-05, 'epoch': 0.4}\n",
    " 44%|████▍     | 1100/2500 [1:49:56<2:43:09,  6.99s/it]{'loss': 0.3462, 'grad_norm': 23.52771759033203, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.44}\n",
    " 48%|████▊     | 1200/2500 [1:58:51<1:39:12,  4.58s/it]{'loss': 0.3688, 'grad_norm': 14.034320831298828, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.48}\n",
    " 52%|█████▏    | 1300/2500 [2:08:30<2:20:24,  7.02s/it]{'loss': 0.2986, 'grad_norm': 7.83740234375, 'learning_rate': 2.4e-05, 'epoch': 0.52}\n",
    " 56%|█████▌    | 1400/2500 [2:17:30<1:20:40,  4.40s/it]{'loss': 0.3507, 'grad_norm': 3.1949851512908936, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.56}\n",
    " 60%|██████    | 1500/2500 [2:26:53<1:52:19,  6.74s/it]{'loss': 0.292, 'grad_norm': 0.4668058156967163, 'learning_rate': 2e-05, 'epoch': 0.6}\n",
    " 64%|██████▍   | 1600/2500 [2:36:36<1:09:43,  4.65s/it]{'loss': 0.2971, 'grad_norm': 8.485663414001465, 'learning_rate': 1.8e-05, 'epoch': 0.64}\n",
    " 68%|██████▊   | 1700/2500 [2:45:45<1:17:54,  5.84s/it]{'loss': 0.3364, 'grad_norm': 0.9123271107673645, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.68}\n",
    " 72%|███████▏  | 1800/2500 [2:55:56<1:23:49,  7.19s/it]{'loss': 0.2954, 'grad_norm': 16.355836868286133, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.72}\n",
    " 76%|███████▌  | 1900/2500 [3:05:11<45:41,  4.57s/it]  {'loss': 0.2849, 'grad_norm': 5.070781707763672, 'learning_rate': 1.2e-05, 'epoch': 0.76}\n",
    " 80%|████████  | 2000/2500 [3:14:58<57:50,  6.94s/it]  {'loss': 0.2646, 'grad_norm': 10.113204956054688, 'learning_rate': 1e-05, 'epoch': 0.8}\n",
    " 84%|████████▍ | 2100/2500 [3:24:44<27:56,  4.19s/it]{'loss': 0.2952, 'grad_norm': 29.44683265686035, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.84}\n",
    " 88%|████████▊ | 2200/2500 [3:34:09<31:48,  6.36s/it]{'loss': 0.272, 'grad_norm': 0.19008371233940125, 'learning_rate': 6e-06, 'epoch': 0.88}\n",
    " 92%|█████████▏| 2300/2500 [3:44:11<16:44,  5.02s/it]{'loss': 0.2581, 'grad_norm': 36.615272521972656, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.92}\n",
    " 96%|█████████▌| 2400/2500 [3:53:24<09:49,  5.90s/it]{'loss': 0.2475, 'grad_norm': 38.397850036621094, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.96}\n",
    "100%|██████████| 2500/2500 [4:03:38<00:00,  6.79s/it]{'loss': 0.3127, 'grad_norm': 13.887683868408203, 'learning_rate': 0.0, 'epoch': 1.0}\n",
    "\n",
    "100%|██████████| 2500/2500 [4:22:57<00:00,  6.31s/it]\n",
    "{'eval_accuracy': 0.909, 'eval_precision': 0.8967117988394584, 'eval_recall': 0.9249800478850758, 'eval_f1': 0.9106265959536437, 'eval_loss': 0.29959747195243835, 'eval_runtime': 1159.081, 'eval_samples_per_second': 4.314, 'eval_steps_per_second': 0.539, 'epoch': 1.0}\n",
    "{'train_runtime': 15777.7598, 'train_samples_per_second': 1.268, 'train_steps_per_second': 0.158, 'train_loss': 0.34220633087158203, 'epoch': 1.0}\n",
    "100%|██████████| 625/625 [19:31<00:00,  1.87s/it]\n",
    "100%|██████████| 3125/3125 [1:37:04<00:00,  1.86s/it]\n",
    "\n",
    "Teacher Validation Metrics: {'eval_accuracy': 0.909, 'eval_precision': 0.8967117988394584, 'eval_recall': 0.9249800478850758, 'eval_f1': 0.9106265959536437, 'eval_loss': 0.29959747195243835, 'eval_runtime': 1174.0596, 'eval_samples_per_second': 4.259, 'eval_steps_per_second': 0.532, 'epoch': 1.0}\n",
    "Teacher Test Metrics: {'eval_accuracy': 0.91036, 'eval_precision': 0.8895724158882053, 'eval_recall': 0.93704, 'eval_f1': 0.9126894455916157, 'eval_loss': 0.29378241300582886, 'eval_runtime': 5827.2636, 'eval_samples_per_second': 4.29, 'eval_steps_per_second': 0.536, 'epoch': 1.0}\n",
    "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "Map (num_proc=14): 100%|██████████| 20000/20000 [01:11<00:00, 278.53 examples/s]\n",
    "Map (num_proc=14): 100%|██████████| 5000/5000 [01:02<00:00, 80.19 examples/s] \n",
    "Map (num_proc=14): 100%|██████████| 25000/25000 [00:48<00:00, 515.84 examples/s]\n",
    "C:\\Users\\maril\\AppData\\Local\\Temp\\ipykernel_24392\\2441323602.py:263: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `DistillationTrainer.__init__`. Use `processing_class` instead.\n",
    "  super().__init__(*args, **kwargs)\n",
    "  4%|▍         | 100/2500 [05:18<2:07:45,  3.19s/it]{'loss': 0.5498, 'grad_norm': 11.526269912719727, 'learning_rate': 4.8e-05, 'epoch': 0.04}\n",
    "  8%|▊         | 200/2500 [10:39<2:02:04,  3.18s/it]{'loss': 0.3948, 'grad_norm': 14.726052284240723, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.08}\n",
    " 12%|█▏        | 300/2500 [15:55<2:01:15,  3.31s/it]{'loss': 0.3972, 'grad_norm': 6.584609031677246, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.12}\n",
    " 16%|█▌        | 400/2500 [21:16<1:51:50,  3.20s/it]{'loss': 0.3456, 'grad_norm': 15.88803482055664, 'learning_rate': 4.2e-05, 'epoch': 0.16}\n",
    " 20%|██        | 500/2500 [26:36<1:46:08,  3.18s/it]{'loss': 0.4028, 'grad_norm': 15.243449211120605, 'learning_rate': 4e-05, 'epoch': 0.2}\n",
    " 24%|██▍       | 600/2500 [31:54<1:40:25,  3.17s/it]{'loss': 0.362, 'grad_norm': 4.344246864318848, 'learning_rate': 3.8e-05, 'epoch': 0.24}\n",
    " 28%|██▊       | 700/2500 [37:13<1:35:25,  3.18s/it]{'loss': 0.4089, 'grad_norm': 25.762807846069336, 'learning_rate': 3.6e-05, 'epoch': 0.28}\n",
    " 32%|███▏      | 800/2500 [42:32<1:29:54,  3.17s/it]{'loss': 0.341, 'grad_norm': 9.24248218536377, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.32}\n",
    " 36%|███▌      | 900/2500 [47:52<1:25:31,  3.21s/it]{'loss': 0.4602, 'grad_norm': 7.068543910980225, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.36}\n",
    " 40%|████      | 1000/2500 [53:11<1:19:51,  3.19s/it]{'loss': 0.3297, 'grad_norm': 11.032055854797363, 'learning_rate': 3e-05, 'epoch': 0.4}\n",
    " 44%|████▍     | 1100/2500 [57:42<1:02:42,  2.69s/it]{'loss': 0.3472, 'grad_norm': 13.0895414352417, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.44}\n",
    " 48%|████▊     | 1200/2500 [1:02:27<49:28,  2.28s/it]  {'loss': 0.3801, 'grad_norm': 6.4442033767700195, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.48}\n",
    " 52%|█████▏    | 1300/2500 [1:07:20<40:38,  2.03s/it]  {'loss': 0.3041, 'grad_norm': 0.9118344187736511, 'learning_rate': 2.4e-05, 'epoch': 0.52}\n",
    " 56%|█████▌    | 1400/2500 [1:11:49<1:03:30,  3.46s/it]{'loss': 0.3498, 'grad_norm': 12.24240779876709, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.56}\n",
    " 60%|██████    | 1500/2500 [1:16:14<48:23,  2.90s/it]  {'loss': 0.2927, 'grad_norm': 14.98419189453125, 'learning_rate': 2e-05, 'epoch': 0.6}\n",
    " 64%|██████▍   | 1600/2500 [1:20:56<36:50,  2.46s/it]{'loss': 0.3233, 'grad_norm': 10.465763092041016, 'learning_rate': 1.8e-05, 'epoch': 0.64}\n",
    " 68%|██████▊   | 1700/2500 [1:25:52<27:45,  2.08s/it]{'loss': 0.306, 'grad_norm': 1.3848687410354614, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.68}\n",
    " 72%|███████▏  | 1800/2500 [1:30:49<41:19,  3.54s/it]{'loss': 0.2774, 'grad_norm': 14.78300666809082, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.72}\n",
    " 76%|███████▌  | 1900/2500 [1:34:57<31:37,  3.16s/it]{'loss': 0.3097, 'grad_norm': 17.732311248779297, 'learning_rate': 1.2e-05, 'epoch': 0.76}\n",
    " 80%|████████  | 2000/2500 [1:39:35<22:35,  2.71s/it]{'loss': 0.2797, 'grad_norm': 9.0259428024292, 'learning_rate': 1e-05, 'epoch': 0.8}\n",
    " 84%|████████▍ | 2100/2500 [1:44:24<14:56,  2.24s/it]{'loss': 0.2965, 'grad_norm': 21.90822410583496, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.84}\n",
    " 88%|████████▊ | 2200/2500 [1:49:23<17:34,  3.52s/it]{'loss': 0.2851, 'grad_norm': 0.12936966121196747, 'learning_rate': 6e-06, 'epoch': 0.88}\n",
    " 92%|█████████▏| 2300/2500 [1:53:39<11:09,  3.35s/it]{'loss': 0.2524, 'grad_norm': 6.887401580810547, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.92}\n",
    " 96%|█████████▌| 2400/2500 [1:58:18<04:43,  2.83s/it]{'loss': 0.2391, 'grad_norm': 23.580942153930664, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.96}\n",
    "100%|██████████| 2500/2500 [2:03:04<00:00,  2.30s/it]{'loss': 0.3137, 'grad_norm': 10.13525390625, 'learning_rate': 0.0, 'epoch': 1.0}\n",
    "\n",
    "100%|██████████| 2500/2500 [2:13:48<00:00,  3.21s/it]\n",
    "{'eval_accuracy': 0.9, 'eval_precision': 0.893025078369906, 'eval_recall': 0.9094173982442139, 'eval_f1': 0.9011466982997232, 'eval_loss': 0.30669721961021423, 'eval_runtime': 644.0439, 'eval_samples_per_second': 7.763, 'eval_steps_per_second': 0.97, 'epoch': 1.0}\n",
    "{'train_runtime': 8028.3603, 'train_samples_per_second': 2.491, 'train_steps_per_second': 0.311, 'train_loss': 0.34195306549072263, 'epoch': 1.0}\n",
    "100%|██████████| 625/625 [10:26<00:00,  1.00s/it]\n",
    "100%|██████████| 3125/3125 [50:24<00:00,  1.03it/s]\n",
    "\n",
    "Student Validation Metrics: {'eval_accuracy': 0.9, 'eval_precision': 0.893025078369906, 'eval_recall': 0.9094173982442139, 'eval_f1': 0.9011466982997232, 'eval_loss': 0.30669721961021423, 'eval_runtime': 627.0832, 'eval_samples_per_second': 7.973, 'eval_steps_per_second': 0.997, 'epoch': 1.0}\n",
    "Student Test Metrics: {'eval_accuracy': 0.90392, 'eval_precision': 0.8891629412671497, 'eval_recall': 0.92288, 'eval_f1': 0.9057077804820601, 'eval_loss': 0.2904188930988312, 'eval_runtime': 3025.1447, 'eval_samples_per_second': 8.264, 'eval_steps_per_second': 1.033, 'epoch': 1.0}\n",
    "\n",
    "Teacher Test: {'eval_accuracy': 0.91036, 'eval_precision': 0.8895724158882053, 'eval_recall': 0.93704, 'eval_f1': 0.9126894455916157, 'eval_loss': 0.29378241300582886, 'eval_runtime': 5827.2636, 'eval_samples_per_second': 4.29, 'eval_steps_per_second': 0.536, 'epoch': 1.0}\n",
    "Student Test: {'eval_accuracy': 0.90392, 'eval_precision': 0.8891629412671497, 'eval_recall': 0.92288, 'eval_f1': 0.9057077804820601, 'eval_loss': 0.2904188930988312, 'eval_runtime': 3025.1447, 'eval_samples_per_second': 8.264, 'eval_steps_per_second': 1.033, 'epoch': 1.0}\n",
    "Student retains about 99.3% of the Teacher's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_trainer.args.disable_tqdm = True\n",
    "distil_trainer.args.disable_tqdm = True\n",
    "\n",
    "#Predictions for Teacher\n",
    "teacher_result = teacher_trainer.predict(teacher_dataset[\"test\"])\n",
    "teacher_logits = teacher_result.predictions\n",
    "teacher_labels = teacher_result.label_ids\n",
    "teacher_preds = np.argmax(teacher_logits, axis=-1)\n",
    "#probability of class=1 for AUC/PR-AUC:\n",
    "teacher_probs = torch.softmax(torch.tensor(teacher_logits), dim=-1).numpy()[:, 1]\n",
    "\n",
    "#Predictions for Student\n",
    "student_result = distil_trainer.predict(distil_dataset[\"test\"])\n",
    "student_logits = student_result.predictions\n",
    "student_labels = student_result.label_ids\n",
    "student_preds = np.argmax(student_logits, axis=-1)\n",
    "#probability of class=1 for AUC/PR-AUC:\n",
    "student_probs = torch.softmax(torch.tensor(student_logits), dim=-1).numpy()[:, 1]\n",
    "\n",
    "def compute_metrics(labels, preds, probs):\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    prec = precision_score(labels, preds)\n",
    "    rec = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    \n",
    "    #Handle AUC/PR-AUC if there's >1 label\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "        pr_auc = average_precision_score(labels, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "        pr_auc = float('nan')\n",
    "    \n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "    return {\n",
    "        \"confusion_matrix\": cm.tolist(),\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1,\n",
    "        \"roc_auc\": auc,\n",
    "        \"pr_auc\": pr_auc,\n",
    "        \"mcc\": mcc\n",
    "    }\n",
    "\n",
    "teacher_metrics = compute_metrics(teacher_labels, teacher_preds, teacher_probs)\n",
    "student_metrics = compute_metrics(student_labels, student_preds, student_probs)\n",
    "\n",
    "print(\"\\n TEACHER TEST METRICS\")\n",
    "for k, v in teacher_metrics.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "print(\"\\n STUDENT TEST METRICS\")\n",
    "for k, v in student_metrics.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE OUTPUT: \n",
    "\n",
    "3162it [2:25:05,  2.75s/it]                         \n",
    "100%|██████████| 3125/3125 [49:14<00:00,  1.06it/s]  \n",
    "\n",
    "TEACHER TEST METRICS \n",
    "confusion_matrix: [[10824, 1676], [1165, 11335]]\n",
    "accuracy: 0.91036\n",
    "precision: 0.8711859196064868\n",
    "recall: 0.9068\n",
    "f1: 0.8886362745482341\n",
    "roc_auc: 0.9570383999999998\n",
    "pr_auc: 0.956745012420531\n",
    "mcc: 0.7733664853464441\n",
    "\n",
    "STUDENT TEST METRICS\n",
    "confusion_matrix: [[10898, 1602], [1529, 10971]]\n",
    "accuracy: 0.90392\n",
    "precision: 0.8725841088045813\n",
    "recall: 0.87768\n",
    "f1: 0.875124636062697\n",
    "roc_auc: 0.9481288064\n",
    "pr_auc: 0.9481079798749482\n",
    "mcc: 0.7495327817416035"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Discussion of the Results\n",
    "\n",
    "The final outcomes show that the teacher network (BERT) has a slightly higher degree of accuracy (approximately 91.03\\%) compared \n",
    "to the student model (DistilBERT) at about 90.39\\%. This gap of roughly one percentage point is interesting given that the teacher \n",
    "network is larger and therefore often more adept at capturing subtle patterns in the training data. I believe it is also \n",
    "important that the student model's accuracy is close to the teacher's accuracy, showing us that distillation transfers much \n",
    "of the teacher’s predictive potential while it is a lighter, more efficient model. As we calculated, the Student model retains \n",
    "about 99.3% of the Teacher's accuracy.\n",
    "\n",
    "By looking at the other metrics, we can see that the teacher’s precision (87.12\\%) and recall (90.68\\%) converge to yield an F1 \n",
    "score of approximately 88.86\\% and that the student has a precision of 87.26\\% and a recall of 87.77\\%, for an F1 score \n",
    "of around 87.51\\%. Although the teacher is higher than the student in recall, the student’s precision is still comparative, and \n",
    "again shows is that the distilled model retains much of the teacher’s capability. The corresponding confusion matrices show that\n",
    "that the teacher model typically commits fewer false negatives, while the student model slightly reduces false positives.\n",
    "\n",
    "In the AUC and PR-AUC results, both networks have high AUC values, where the teacher is roughly 0.957 and student is about 0.948, \n",
    "showing their robustness. Additionally, the teacher achieves an MCC of around 0.773, while the student registers about 0.750. This \n",
    "is interesting because although the teacher maintains a small advantage in predictive capability, the student still operates \n",
    "in a robust and reliable manner.\n",
    "\n",
    "These metrics tell us that the distilled student model retains the core strength of the teacher model being able to achieve \n",
    "similar performance on most metrics while it has a reduced complexity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
